{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <table><tr><td><img src=\"images/dbmi_logo.png\" width=\"75\" height=\"73\" alt=\"Pitt Biomedical Informatics logo\"></td><td><img src=\"images/pitt_logo.png\" width=\"75\" height=\"75\" alt=\"University of Pittsburgh logo\"></td></tr></table>\n",
    " \n",
    " \n",
    " # Social Media and Data Science - Part 5\n",
    " \n",
    " \n",
    "Data science modules developed by the University of Pittsburgh Biomedical Informatics Training Program with the support of the National Library of Medicine data science supplement to the University of Pittsburgh (Grant # T15LM007059-30S1). \n",
    "\n",
    "Developed by Harry Hochheiser, harryh@pitt.edu. All errors are my responsibility.\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Use social media posts to explore the appplication of text and natural language processing to see what might be learned from online interactions.\n",
    "\n",
    "Specifically, we will retrieve, annotate, process, and interpret Twitter data on health-related issues such as smoking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "References:\n",
    "* [Mining Twitter Data with Python (Part 1: Collecting data)](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)\n",
    "* The [Tweepy Python API for Twitter](http://www.tweepy.org/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "D:\\Anaconda\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from spacy.symbols import ORTH, LEMMA, POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final part of our journey through social media data retrieval, annotation, natural langauge processing, and classififcation will challenge you to apply these techniques to a new problem. Specifically, you will create, annotate, and process a new data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0.1 Setup\n",
    "\n",
    "As before, we start with the Tweets class and the configuration for our Twitter API connection.  We may not need this, but we'll load it in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",tweet_mode='extended',count=corpus_size)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(30)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def combineTweets(self,other):\n",
    "        for otherid in other.getIds():\n",
    "            tweet = other.getTweet(otherid)\n",
    "            searchTerm = other.getSearchTerm(otherid)\n",
    "            searchTime = other.getSearchTime(otherid)\n",
    "            self.addTweet(tweet,searchTime,searchTerm)\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text\n",
    "                \n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "   \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    " \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' in tweet:\n",
    "            return tweet['codes']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # NEW -ROUTINE TO GET PROFILE\n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        for id in self.tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the values of your keys into these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'hide'\n",
    "consumer_secret = 'hide'\n",
    "access_token = 'hide'\n",
    "access_secret = 'hide'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also load some routines that we defined in [Part 3](SocialMedia - Part 3.ipynb):\n",
    "    \n",
    "1. Our routine for creating a customized NLP pipeline\n",
    "2. Our routine for including tokens\n",
    "3. The `filterTweetTokens` routine defined in an exercise (Without the inclusion of named entities. It will be easier to leave them out for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwitterNLP():\n",
    "    nlp = spacy.load('en')\n",
    "    \n",
    "    for word in nlp.Defaults.stop_words:\n",
    "        lex = nlp.vocab[word]\n",
    "        lex.is_stop = True\n",
    "    \n",
    "    special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "    nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "    nlp.tokenizer.add_special_case(u'E-cigarette', special_case)\n",
    "    vape_case = [{ORTH: u'vape',LEMMA:u'vape',POS: u'NOUN'}]\n",
    "    \n",
    "    vape_spellings =[u'vap',u'vape',u'vaping',u'vapor',u'Vap',u'Vape',u'Vapor',u'Vapour']\n",
    "    for v in vape_spellings:\n",
    "        nlp.tokenizer.add_special_case(v, vape_case)\n",
    "    def hashtag_pipe(doc):\n",
    "        merged_hashtag = True\n",
    "        while merged_hashtag == True:\n",
    "            merged_hashtag = False\n",
    "            for token_index,token in enumerate(doc):\n",
    "                if token.text == '#':\n",
    "                    try:\n",
    "                        nbor = token.nbor()\n",
    "                        start_index = token.idx\n",
    "                        end_index = start_index + len(token.nbor().text) + 1\n",
    "                        if doc.merge(start_index, end_index) is not None:\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "                    except:\n",
    "                        pass\n",
    "        return doc\n",
    "    nlp.add_pipe(hashtag_pipe,first=True)\n",
    "    return nlp\n",
    "\n",
    "def includeToken(tok):\n",
    "    val =False\n",
    "    if tok.is_stop == False:\n",
    "        if tok.is_alpha == True: \n",
    "            if tok.text =='RT':\n",
    "                val = False\n",
    "            elif tok.pos_=='NOUN' or tok.pos_=='PROPN' or tok.pos_=='VERB':\n",
    "                val = True\n",
    "        elif tok.text[0]=='#' or tok.text[0]=='@':\n",
    "            val = True\n",
    "    if val== True:\n",
    "        stripped =tok.lemma_.lower().strip()\n",
    "        if len(stripped) ==0:\n",
    "            val = False\n",
    "        else:\n",
    "            val = stripped\n",
    "    return val\n",
    "\n",
    "def filterTweetTokens(tokens):\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will include some additional modules from Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to go along for an exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the source of social media comments might be an important step in the process of interpreting a large corpus. Continuing with our example of smoking and vaping, it might be interesting to compare tweets from users - people who are talking about their own personal use  to those who might be either promoting vaping  (manufacturers, sponsors, etc.) or warning about dangers of vaping (physicians, researchers, public health agencies, etc.).\n",
    "\n",
    "A team of researchers at RTI International tackled this problem in a 2018 paper [Classification of Twitter Users Who Tweet About E-Cigarettes](http://publichealth.jmir.org/2017/3/e63/) by Annice Kim and colleagues collected tweets and attributed them to individuals, enthusiasts, \"informed agencies (news media or health community), marketers, or spammers. \n",
    "\n",
    "Your goal here is to collect a small data set and to attempt a smaller version of this challenge. Specifically, we will try to collect preliminary data for a classifier capable of identifing tweets from users of e-cigarettes vs. others.  Using any of the code found in Parts 1-4, complete these steps:\n",
    "\n",
    "1. Run some searches for tweets like 'e-cig', 'e-cigarette', 'vape' and 'vaping'. Collect a corpus of 200-300  or more tweets. You might want to save each of these result sets in files.\n",
    "\n",
    "2. Combine these tweets into one large collection using the 'Tweet' class listed above. Save the results in a file \n",
    "\n",
    "3. Annotate 50 of these tweets as pertaining to either 'individual' or 'non-individual'. Be sure that you do at least a few of the tweets from each of the original sets. One way to do this might be to randomize the tweets. Save the annotated results in a file. \n",
    "\n",
    "4.Review at the distrbution. Is it close to even? If not, do more.\n",
    "\n",
    "5. Take your annotated tweets - split them into train (80%) and test (20%) sets.  Process the train data and build a model (based on a TfIdf Vectorizer and an SVM). Evaluate the model on the test data sets.\n",
    "\n",
    "6. Test your model on the remaining tweets. What does your result look like?\n",
    "\n",
    "7. Review some of the data to identify opportunities for improvement - how might you make these models bettter?\n",
    "\n",
    "8. Reflect on the reproducibility and the reusability of the code: what should be done to make these tools easier to apply to other datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "*ANSWER FOLLOWS - insert answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1. Run some searches for tweets like 'e-cig', 'e-cigarette', 'vape' and 'vaping'. Collect a corpus of 200-300 or more tweets. You might want to save each of these result sets in files.\n",
    "Te_cig = Tweets(\"e-cig\",60)\n",
    "Te_cigarette= Tweets(\"e-cigarette\",60)\n",
    "Tvape=Tweets(\"vape\",60)\n",
    "Tvaping=Tweets(\"vaping\",60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Te_cig.saveTweets('tweets_ecig.json')\n",
    "Te_cigarette.saveTweets('tweets_ecigarette.json')\n",
    "Tvape.saveTweets('tweets_vape.json')\n",
    "Tvaping.saveTweets('tweets_vaping.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2. Combine these tweets into one large collection using the 'Tweet' class listed above. Save the results in a file\n",
    "tweets_set=Tweets()\n",
    "tweets_set.combineTweets(Te_cigarette)\n",
    "tweets_set.combineTweets(Te_cig)\n",
    "tweets_set.combineTweets(Tvape)\n",
    "tweets_set.combineTweets(Tvaping)\n",
    "tweets_set.saveTweets('tweets_set.json')\n",
    "# print(tweets_set.countTweets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## once ruing the below for loop twice, please rerun code from here.\n",
    "tweetsS=Tweets()\n",
    "tweetsS.readTweets(\"tweets_set.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3. Annotate 50 of these tweets as pertaining to either 'individual' or 'non-individual'. Be sure that you do at least a few of the tweets from each of the original sets. One way to do this might be to randomize the tweets. Save the annotated results in a file.\n",
    "ids=list(tweetsS.getIds())\n",
    "# len(ids)\n",
    "annotateCodes=['individual','non-individual']\n",
    "ids50=random.sample(ids,50)\n",
    "for id in ids50:\n",
    "    tweetsS.addCode(id, random.choice(annotateCodes))\n",
    "\n",
    "# print(tweetsS.countTweets())\n",
    "## please be careful to rain this for loop, if runing more than once, the annotated list may be destroyed. which is not only in one category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'non-individual'}\n",
      "{'individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'individual'}\n",
      "{'non-individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'non-individual'}\n",
      "{'individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'non-individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'individual'}\n",
      "{'non-individual'}\n",
      "{'individual'}\n",
      "{'non-individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'individual'}\n",
      "{'non-individual'}\n",
      "{'non-individual'}\n",
      "{'individual'}\n",
      "{'non-individual'}\n"
     ]
    }
   ],
   "source": [
    "# tweetsS.getCodes(ids50[0])\n",
    "# tweetsS.getCodes(ids50[19])\n",
    "\n",
    "# codeAll=[cat[1] for cat in tweetsS]\n",
    "# print('number of individual:', tweetsS.count('individual'))\n",
    "\n",
    "for i in range(50):\n",
    "    td=ids50[i]\n",
    "    print(tweetsS.getCodes(td))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "print(tweetsS.countTweets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsS.saveTweets('tweets_annotated.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "('non-individual', 25)\n",
      "('individual', 25)\n",
      "According to the result, this is close to even\n"
     ]
    }
   ],
   "source": [
    "###4. Review at the distrbution. Is it close to even? If not, do more.\n",
    "\n",
    "totalTweets=Tweets()\n",
    "totalTweets.readTweets('tweets_annotated.json')\n",
    "tweets_annotated50 = Tweets()\n",
    "all_id = list(totalTweets.getIds())\n",
    "for tweet_id in all_id:\n",
    "    tweet = totalTweets.getTweet(tweet_id)\n",
    "    if 'codes' in tweet:\n",
    "        tweets_annotated50.addTweet(tweet,totalTweets.getSearchTime(tweet_id), totalTweets.getSearchTerm(tweet_id))\n",
    "print(tweets_annotated50.countTweets())\n",
    "tweets_annotated50.saveTweets('tweets_annotated50.json')\n",
    "print(tweets_annotated50.getCodeProfile()[0])\n",
    "print(tweets_annotated50.getCodeProfile()[1])\n",
    "print('According to the result, this is close to even')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "##5. Take your annotated tweets - split them into train (80%) and test (20%) sets. Process the train data and build a model (based on a TfIdf Vectorizer and an SVM). Evaluate the model on the test data sets.\n",
    "def flattenTweets(tweets):\n",
    "    flat=[]\n",
    "    for i in tweets.getIds():\n",
    "        text = tweets.getText(i)\n",
    "        cat = tweets.getCodes(i) \n",
    "        pair =(text,cat)\n",
    "        flat.append(pair)\n",
    "    return flat\n",
    "\n",
    "\n",
    "def getTestTrainSplit(pairs,splitFactor=0.8):\n",
    "    random.shuffle(pairs)\n",
    "    split=int(len(pairs)*splitFactor)\n",
    "    train=pairs[:split]\n",
    "    test =pairs[split:]\n",
    "    return train,test\n",
    "\n",
    "def tokenizeText(text):\n",
    "    nlp=getTwitterNLP()\n",
    "    tokens=nlp(text)\n",
    "    return filterTweetTokens(tokens)\n",
    "\n",
    "def getTestTrain(tweets,splitFactor=0.8):\n",
    "    tweets = flattenTweets(tweets)\n",
    "    train,test=getTestTrainSplit(tweets,splitFactor)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ids50:\n",
    "#     print(tweets_annotated50.getCodes(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 10\n"
     ]
    }
   ],
   "source": [
    "tweets_annotated50=Tweets()\n",
    "tweets_annotated50.readTweets(\"tweets_annotated50.json\")\n",
    "flatAnnotatedT = flattenTweets(tweets_annotated50)\n",
    "train,test = getTestTrainSplit(flatAnnotatedT)\n",
    "print(str(len(train))+ \" \"+str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTexts,trainCats=zip(*train)\n",
    "testTexts,testCats=zip(*test)\n",
    "\n",
    "## There are two methods  (first)\n",
    "# trainCats = [list(i)[0] for i in trainCats]\n",
    "# testCats = [list(i)[0] for i in testCats]\n",
    "# trainCats=list(trainCats)\n",
    "# testCats=list(testCats)\n",
    "\n",
    "## second\n",
    "trainCats = [list(i) for i in trainCats]\n",
    "testCats = [list(i) for i in testCats]\n",
    "\n",
    "trainCats=np.array(trainCats) ## we can add these two lines code to avoid pipe.fit 1dim warning.\n",
    "trainCats=trainCats.reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function <...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer= TfidfVectorizer(tokenizer=tokenizeText,preprocessor=lambda x: x)\n",
    "clf = LinearSVC()\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('clf', clf)])\n",
    "\n",
    "\n",
    "# trainCats=list(trainCats)   not corrected data type\n",
    "# trainTexts=list(trainTexts)\n",
    "pipe.fit(trainTexts,trainCats)\n",
    "\n",
    "# type(testTexts)\n",
    "# type(trainCats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "##6. Test your model on the remaining tweets. What does your result look like?\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "preds = pipe.predict(testTexts)\n",
    "\n",
    "def convertToNumeric(cats):\n",
    "    nums =[]\n",
    "    for c in cats:\n",
    "        if c =='non-individual':\n",
    "            nums.append(1)\n",
    "        elif c=='individual': \n",
    "            nums.append(-1)\n",
    "    return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, 1, -1, -1, 1, 1, -1, 1, 1]\n",
      "[1, -1, 1, 1, -1, 1, 1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "## if use list(i) for i in trainCats, then don't need to run this cell\n",
    "testCats=np.array(testCats)\n",
    "testCats\n",
    "numCats=convertToNumeric(testCats)\n",
    "print(numCats)\n",
    "print(numPreds)\n",
    "# preds\n",
    "# print(type(numCats))\n",
    "# print(type(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.6\n",
      "Precision is [0.6 0.6]\n",
      "Recall is [0.6 0.6]\n"
     ]
    }
   ],
   "source": [
    "# numCats=convertToNumeric(testCats)\n",
    "numPreds=convertToNumeric(preds)\n",
    "\n",
    "print(\"Accuracy is\", accuracy_score(testCats, preds))\n",
    "print(\"Precision is \"+str(precision_score(numCats,numPreds,average=None)))\n",
    "print(\"Recall is \"+ str(recall_score(numCats,numPreds,average=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Review some of the data to identify opportunities for improvement - how might you make these models bettter?\n",
    "\n",
    "In my opinion, one method to improve this model is that we can use \"search terms\", such as vaping and vape, as features to train the models. In this way, I may get more predictors and have higher model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Reflect on the reproducibility and the reusability of the code: what should be done to make these tools easier to apply to other datasets.\n",
    "\n",
    "To make the code easier to apply to other datasets, we have to make some modifications in current functions and classes, to fit different datasets. In addition, the current code focuses more on text using nlp techniques. If there are video files needed to be processed, our code may not work well. Thus, we need to analyze the types of datasets and the reults we want to get before we apply exising code to other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END ANSWER*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
